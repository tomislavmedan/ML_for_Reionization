{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#using GPU, comment out if on CPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow \n",
    "# prerequisites\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras import objectives\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import concatenate as concat\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate,Reshape,Flatten\n",
    "from tensorflow.keras.layers import Reshape,UpSampling3D,RepeatVector,Conv3D,MaxPool3D\n",
    "import tensorflow.keras\n",
    "tensorflow.keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "#these are all chosen to make the overall distrubtion roughly unit gaussian... \n",
    "def density_transform(x):\n",
    "    return np.log10(3*x)\n",
    "\n",
    "def velocity_transform(x):\n",
    "    return x/75.0\n",
    "\n",
    "def temp_transform(x):\n",
    "    return (3.82-np.log10(x.clip(max=500000))*3.8) +10\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs_x,list_IDs_y, normy = True,\n",
    "                 to_fit=True, batch_size=32, \n",
    "                 n_channels=1, shuffle=True, inde = [0,1,2],\n",
    "                 max_num = 10, fix_direction=False):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        \n",
    "        :param to_fit: True to return X and y, False to return X only (I think always should be true?)\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param n_channels: number of image channels (should be 1)\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs_x = list_IDs_x #filenames of density fields\n",
    "        self.list_IDs_y = list_IDs_y #filenames of hydro fields\n",
    "        self.max_num = max_num #maximum number of files to use per epoch\n",
    "        self.normy = normy #normalize hydrofield quantities (y/n)?\n",
    "        self.to_fit = to_fit #always True? False not implemented well...\n",
    "        self.batch_size = batch_size \n",
    "        self.n_channels = n_channels #should always be one\n",
    "        self.shuffle = shuffle #shuffle order of boxes\n",
    "        self.st = \"ijk\" #for using various reflection symmetries \n",
    "        self.max = self.__len__()\n",
    "        self.n = 0 #intializing get_item for random rearrangemnt\n",
    "        self.fix_direction = fix_direction #use mirror/reflection symmetries\n",
    "        self.perm = list(itertools.permutations(inde))\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs_x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        #choose random permulation\n",
    "        \n",
    "        #Choose orientation...\n",
    "        y = self.y[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        X  = self.X[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        if self.fix_direction:\n",
    "            ore = (0,1,2)\n",
    "        else:\n",
    "            ore = random.choice(self.perm)\n",
    "            #print(ore)\n",
    "     #   ore = (0, 1, 2)\n",
    "        #print(ore)\n",
    "        # Generate data\n",
    "        #X = self._perm_x(X,ore)\n",
    "        X = X\n",
    "        if self.to_fit:\n",
    "            #print(y.shape)\n",
    "            #y = self._perm_y(y,ore)\n",
    "            y = y\n",
    "            if self.normy:\n",
    "                y = self._norm(y[:,[0,1,4],:,:])\n",
    "            else:\n",
    "                y = y[:,[0,1,4],:,:]\n",
    "            return [y,X],y\n",
    "\n",
    "\n",
    "    def _perm_x(self,hold,i):\n",
    "        #for mirror/reflection symmetries\n",
    "        Q = self.st[i[0]]+self.st[i[1]]+self.st[i[2]]\n",
    "       \n",
    "        t1_x_temp = np.einsum('mnijk->mn'+Q, hold)\n",
    "        t1_x_temp[:,[0,1, 2,3],:,:,:] = t1_x_temp[:,[0,i[0]+1,i[1]+1,i[2]+1],:,:,:]\n",
    "        return t1_x_temp\n",
    "    \n",
    "    def _perm_y(self,hold,i):\n",
    "        #for mirror/reflection symmetries\n",
    "\n",
    "        Q = self.st[i[0]]+self.st[i[1]]+self.st[i[2]]\n",
    "\n",
    "\n",
    "        t1_y_temp = np.einsum('mnijk->mn'+Q, hold)\n",
    "        t1_y_temp[:,[0,1, 2,3,4],:,:,:] = t1_y_temp[:,[0,i[0]+1,i[1]+1,i[2]+1,4],:,:,:]\n",
    "        return t1_y_temp\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs_x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "                # Generate indexes of the batch\n",
    "        #print(indexes)\n",
    "        indexes = self.indexes[0:self.max_num]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_tempx = [self.list_IDs_x[k] for k in indexes]\n",
    "        list_IDs_tempy = [self.list_IDs_y[k] for k in indexes]\n",
    "\n",
    "        self.X = self._generate_X(list_IDs_tempx)\n",
    "        self.y = self._generate_y(list_IDs_tempy)\n",
    "\n",
    "            \n",
    "    def _generate_X(self, list_IDs_temp_x):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        test_x = []\n",
    "        for i in list_IDs_temp_x:\n",
    "            test_x.append([np.load(i)])\n",
    "       \n",
    "        test_x = np.reshape(np.array(test_x),(-1,4,32,32,32)) #JUST CHANGED FROM 16^3\n",
    "        return test_x\n",
    "\n",
    "    def _generate_y(self, list_IDs_temp_y):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        test_y = []\n",
    "        for i in list_IDs_temp_y:\n",
    "            test_y.append([np.load(i)[:,:,:,:]])\n",
    "        test_y = np.reshape(np.array(test_y),(-1,4096,32,32,32)) ##MATCHED TO FIT MY DATA\n",
    "                            \n",
    "        test_yn = test_y[:,:,:,:] #selecting only one baryon velocity\n",
    "        return test_yn\n",
    "#reducing variance so all variables have similar dynamic range: otherwise loss will be ->inf :'()'\n",
    "\n",
    "    def _norm(self,test_yn):\n",
    "        #if normed = True\n",
    "            \n",
    "            test_yn[:,0] = density_transform(test_yn[:,0].clip(min=0.0001))\n",
    "            test_yn[:,1] = velocity_transform(test_yn[:,1])\n",
    "            test_yn[:,2] = temp_transform(test_yn[:,2].clip(min=0.0001))\n",
    "            \n",
    "            \n",
    "            return test_yn\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "            self.n = 0\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D,Flatten,Conv3DTranspose,concatenate\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate,Reshape,Flatten\n",
    "from tensorflow.keras.layers import Reshape,UpSampling3D,RepeatVector,Conv3D,MaxPool3D, AvgPool3D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import tensorflow.keras\n",
    "\n",
    "\n",
    "class HyPhy:\n",
    "    \n",
    "    \"\"\"\n",
    "    Class that holds the model for both train (cvae, and associated loss) and generation (gen),\n",
    "    \n",
    "    Currently takes as input just number of hidden dimensions and n_hidden, could \n",
    "    easily make more things free parameters to set...\n",
    "    \n",
    "    Set up slightly strangly to allow (hopefully) seamless switching between training and \n",
    "    generation, as well as changing generation size...\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=128,z_dim=27, edge_clip=10, rec_loss_factor = 1):\n",
    "        self.n_hidden = n_hidden #size of dense layers used to set mu, logvar for latent space\n",
    "        self.z_dim = z_dim #number of latent space dimensions\n",
    "        self.edge_clip = edge_clip #pixels to clip off of reconstructed tau for comparison\n",
    "        self.rec_loss_factor = rec_loss_factor #relative weight of kl loss vs. rec loss\n",
    "        \n",
    "        self.__init_encoder__() #initialize encoder layers\n",
    "        self.__init_decoder__() #initialize decoder layers\n",
    "        \n",
    "        self.__init_cvae__() #creates training model\n",
    "        #self.__init_gen__() #just call HyPhy.gen(), not made by default\n",
    "        \n",
    "    def __init_encoder__(self):\n",
    "        ## dm\n",
    "     \n",
    "        self.inter_up0 = Conv3D(4,3,padding=\"same\",activation=\"selu\")\n",
    "        self.inter_up0p = Conv3D(5,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm1 = Conv3D(6,2,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm1p = Conv3D(6,3,padding=\"same\",activation=\"selu\")\n",
    "\n",
    "        self.encode_dm2 = Conv3D(8,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm3 = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm4 = Conv3D(10,2,padding=\"same\",activation=\"selu\") ## linear?\n",
    "\n",
    "        ## tau\n",
    "    \n",
    "        self.encode_tau1 = Conv3D(3,2,padding=\"same\") \n",
    "        self.encode_tau1P = Conv3D(4,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau2 = Conv3D(6,4,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau2P = Conv3D(8,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau3 = Conv3D(10,5,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau3P = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau4 = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau4p = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "    def __init_decoder__(self):\n",
    "        \n",
    "        self.z_decoder2 = Conv3DTranspose(16,6,padding=\"same\",activation=\"selu\")#Dense(n_hidden*3, activation='relu')\n",
    "        self.z_decoder2p = Conv3D(16,3,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3 = Conv3DTranspose(14,3,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3_mix = Conv3DTranspose(14,1,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3P = Conv3D(14,2,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder4 = Conv3DTranspose(12,1,padding=\"same\",activation=\"selu\")#Dense(n_hidden*2, activation='linear')\n",
    "        self.z_decoder4P = Conv3D(12,2,padding=\"same\",activation=\"selu\")#Dense(n_hidden*2, activation='linear')\n",
    "\n",
    "        self.y_decoder = Conv3D(10,1,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "       # self.y_decoder_BN = BatchNormalization()\n",
    "        self.y_decoderP1 = Conv3D(8,5,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP2 = Conv3D(6,3,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP2_mix = Conv3D(10,1,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP3 = Conv3D(6,3,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP4 = Conv3D(3,3,padding=\"same\")#Dense(x_tr.shape[1], activation='linear')\n",
    "    \n",
    "    def encoder_dm(self,dm_box):\n",
    "        \n",
    "        #getting dm field to same size as tau field...\n",
    "        dm0 = UpSampling3D(name=\"dm_up1\")(dm_box)\n",
    "        dm0 = self.inter_up0(dm0)\n",
    "        dm0 = self.inter_up0p(dm0)\n",
    "        dm0 = UpSampling3D(name=\"dm_up2\")(dm0)\n",
    "        \n",
    "        #step one\n",
    "        dm1 = self.encode_dm1(dm0)\n",
    "        dm1 = self.encode_dm1p(dm1)\n",
    "        dm1_p = MaxPool3D()(dm1)\n",
    "\n",
    "        #step two\n",
    "        dm2 = self.encode_dm2(dm1_p)\n",
    "        dm2_p = MaxPool3D()(dm2)\n",
    "      \n",
    "        #step three\n",
    "        dm3 = self.encode_dm3(dm2_p)\n",
    "        dm3_p = MaxPool3D()(dm3)\n",
    "\n",
    "        #step four\n",
    "        dm4 = self.encode_dm4(dm3_p)\n",
    "        dm4 = MaxPool3D()(dm4)\n",
    "        return dm0,dm1,dm2,dm3,dm4\n",
    "    \n",
    "    def encoder_tau(self,x_in,dm0,dm1,dm2,dm3,dm4):\n",
    "        \n",
    "        tau_box = Reshape((3,64,64,64))(x_in)\n",
    "\n",
    "        #step 1\n",
    "        tau1 = self.encode_tau1(tau_box)\n",
    "        tau1 = self.encode_tau1P(tau1)\n",
    "\n",
    "        #merge + step 2\n",
    "        tau1_dm1 = concatenate([dm1,tau1],axis=1)\n",
    "        tau2 = self.encode_tau2(tau1_dm1)\n",
    "        tau2 = self.encode_tau2P(tau2)\n",
    "        tau2_p = MaxPool3D()(tau2)\n",
    "\n",
    "        #merge + step 3\n",
    "\n",
    "        tau2_dm2 = concatenate([dm2,tau2_p],axis=1)\n",
    "        tau3 = self.encode_tau3(tau2_dm2)\n",
    "        tau3 = self.encode_tau3P(tau3)\n",
    "        tau3 = BatchNormalization()(tau3)\n",
    "        tau3_p = MaxPool3D()(tau3)\n",
    "\n",
    "        #merge + step 4\n",
    "        tau3_dm3 = concatenate([dm3,tau3_p],axis=1)\n",
    "        tau4 = self.encode_tau4(tau3_dm3)\n",
    "        tau4 = MaxPool3D()(tau4) #maybe do something else here? more layers?\n",
    "       # tau4 = self.encode_tau4p(tau4)\n",
    "        tau4 = MaxPool3D()(tau4)\n",
    "        return tau4\n",
    "        \n",
    "    def variational_block(self,tau4,dm4):\n",
    "        \n",
    "        x_encoded = concatenate([tau4,dm4])\n",
    "        x_encoded = Flatten()(x_encoded)\n",
    "        x_encoded = Dense(self.n_hidden, activation='selu')(x_encoded)\n",
    "        x_encoded = Dropout(.10)(x_encoded)\n",
    "        x_encoded = Dense(self.n_hidden//2, activation='selu')(x_encoded)\n",
    "        \n",
    "        self.mu = Dense(self.z_dim, activation='linear')(x_encoded)\n",
    "        self.log_var = Dense(self.z_dim, activation='linear')(x_encoded)\n",
    "        \n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            eps = K.random_normal(shape=(K.shape(dm4)[0], self.z_dim), mean=0., stddev=1.0)\n",
    "            return mu + K.exp(log_var/2.) * eps\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.z_dim,))([self.mu, self.log_var])\n",
    "        return z\n",
    "    \n",
    "    def decoder(self,z_new, dm0,dm1,dm2,dm3,dm4):\n",
    "            \n",
    "        #dm4_new = Reshape((10,4,4,4))(dm4)\n",
    "\n",
    "        z_cond = concatenate([z_new, dm4],axis=1)\n",
    "    \n",
    "        z_decoded = self.z_decoder2(z_cond)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "        z_decoded = self.z_decoder2p(z_decoded)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm3],axis=1)\n",
    "        z_decoded = self.z_decoder3(z_decoded)\n",
    "        z_decoder = self.z_decoder3_mix(z_decoded)\n",
    "        z_decoded = self.z_decoder3P(z_decoded)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm2],axis=1)\n",
    "\n",
    "        z_decoded = self.z_decoder4(z_decoded)\n",
    "        z_decoded = self.z_decoder4P(z_decoded)\n",
    "\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm1],axis=1)\n",
    "\n",
    "        y0 = self.y_decoder(z_decoded)\n",
    "       # y0 = self.y_decoder_BN(y0)\n",
    "        z_decoded = concatenate([y0,dm0],axis=1)\n",
    "        y0 = self.y_decoderP1(y0)\n",
    "        y0 = self.y_decoderP2(y0)\n",
    "        y0 = self.y_decoderP2_mix(y0)\n",
    "        y0 = self.y_decoderP3(y0)\n",
    "        y = self.y_decoderP4(y0)\n",
    "        return y\n",
    "    \n",
    "    def __init_cvae__(self):\n",
    "        \n",
    "        condition = Input(shape=(4,16,16,16),name=\"DM_field\")\n",
    "        x_in = Input(shape=(3,64,64,64),name=\"tau_field\")\n",
    "        dm_box = Reshape((4,16,16,16))(condition)\n",
    "\n",
    "        dm0,dm1,dm2,dm3,dm4 = self.encoder_dm(dm_box)\n",
    "        tau4 = self.encoder_tau(x_in,dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        z = self.variational_block(tau4,dm4)\n",
    "        \n",
    "        z_new = Lambda(lambda x: tensorflow.tensordot(x, K.ones((4,4,4)), axes=0))(z)\n",
    "        \n",
    "        y = self.decoder(z_new, dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        self.cvae = Model([x_in, condition], y)\n",
    "    \n",
    "    def cvae_loss(self,x,y):\n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "\n",
    "        reconstruction_loss = tensorflow.compat.v1.metrics.mean_squared_error(x_f, y_f)*16**3*4**3*self.rec_loss_factor\n",
    "        kl_loss = 0.5 * K.sum(K.square(self.mu) + K.exp(self.log_var) - self.log_var - 1, axis = -1)\n",
    "        loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return loss\n",
    "    \n",
    "    def kl_loss(self,x,y):\n",
    "        # just for debugging\n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        kl_loss = 0.5 * K.sum(K.square(self.mu) + K.exp(self.log_var) - self.log_var - 1, axis = -1)\n",
    "        return kl_loss\n",
    "    \n",
    "    def rec_loss(self,x,y):    \n",
    "    \n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "\n",
    "        reconstruction_loss = tensorflow.compat.v1.metrics.mean_squared_error(x_f, y_f)*16**3*4**3*self.rec_loss_factor\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def generator(self,size1 = 32,reduced_len=8):\n",
    "        \n",
    "        _condition = Input(shape=(4,size1,size1,size1),name=\"DM_field\")\n",
    "        _z = Input(shape=(self.z_dim,))\n",
    "        #x_in = Input(shape=(3,64,64,64),name=\"tau_field\")\n",
    "        \n",
    "        dm0,dm1,dm2,dm3,dm4 = self.encoder_dm(_condition)\n",
    "        \n",
    "        _z_new = Lambda(lambda x: tensorflow.tensordot(x, K.ones((reduced_len,reduced_len,reduced_len)), axes=0))(_z)\n",
    "\n",
    "        _y = self.decoder(_z_new, dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        self.gen = Model([_condition,_z], _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing of HyPhy_ density files\n",
    "\n",
    "#Below are two functions used in the pre-processing of my density fields BEFORE\n",
    "#they are saved to .npy files\n",
    "\n",
    "def scale_field(field_in, field_type, inv=False):\n",
    "    '''\n",
    "    function to scale Lagrangian, Eulerian, and reionization fields\n",
    "    from their physical units to the range (0,1), and invert this scaling if needed.\n",
    "    Can choose other scalings from e.g. https://en.wikipedia.org/wiki/Feature_scaling - right now does min-max scaling.\n",
    "    Keyword arguments:\n",
    "    field_in -- the field to scale\n",
    "    field_type -- type of field: 'lag', 'eul', or 'reion'\n",
    "    inv -- False=forward scaling from physical units to normalized, True=inverse\n",
    "    \"\"\"\n",
    "    '''\n",
    "    field_types =  ['eul', 'lag', 'reion']\n",
    "    field_mins  = np.array([ -1.        , -23.45174217,   6.69999981])\n",
    "    field_maxs  = np.array([11.14400196, 22.52767944, 16.        ])\n",
    "    ind = field_types.index(field_type)\n",
    "    fmin = field_mins[ind]\n",
    "    fmax = field_maxs[ind]\n",
    "    #min max scaling\n",
    "    if not inv:\n",
    "        return (field_in - fmin)/(fmax - fmin)\n",
    "    if inv:\n",
    "        return field_in * (fmax - fmin) + fmin\n",
    "    \n",
    "def cubify(arr, newshape):\n",
    "    '''stolen from https://stackoverflow.com/questions/42297115/numpy-split-cube-into-cubes'''\n",
    "    oldshape = np.array(arr.shape)\n",
    "    repeats = (oldshape / newshape).astype(int)\n",
    "    tmpshape = np.column_stack([repeats, newshape]).ravel()\n",
    "    order = np.arange(len(tmpshape))\n",
    "    order = np.concatenate([order[::2], order[1::2]])\n",
    "    \n",
    "    # newshape must divide oldshape evenly or else ValueError will be raised\n",
    "    return arr.reshape(tmpshape).transpose(order).reshape(-1, *newshape)\n",
    "\n",
    "def uncubify(arr, oldshape):\n",
    "    '''stolen from https://stackoverflow.com/questions/42297115/numpy-split-cube-into-cubes'''\n",
    "    N, newshape = arr.shape[0], arr.shape[1:]\n",
    "    oldshape = np.array(oldshape)    \n",
    "    repeats = (oldshape / newshape).astype(int)\n",
    "    tmpshape = np.concatenate([repeats, newshape])\n",
    "    order = np.arange(len(tmpshape)).reshape(2, -1).ravel(order='F')\n",
    "    return arr.reshape(tmpshape).transpose(order).reshape(oldshape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "#Here we have the data processing I used on my density files to turn it \n",
    "#into files of size (n_files,4096,32,32,32)\n",
    "\n",
    "#NO NEED TO RUN THIS AGAIN\n",
    "datadir = '/global/cscratch1/sd/tmedan/notebooks/'\n",
    "def data_feeder(n_files, img_shape):\n",
    "    sim_size  =  512\n",
    "    n_subcube = int((sim_size//img_shape)**3)\n",
    "    # make empty array first.\n",
    "    densE = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    densL = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    reion = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    for i in range(n_files):\n",
    "        # load in simulation\n",
    "        densEfile_i = datadir+'density_Eul/dens_{:02d}'.format(i)\n",
    "        densE_i = np.fromfile(open(densEfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        \n",
    "        densLfile_i = datadir+'density_Lag/dens_{:02d}'.format(i)\n",
    "        densL_i = np.fromfile(open(densLfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        \n",
    "        reionfile_i = datadir+'reionization/reion_{:02d}'.format(i)\n",
    "        reion_i = np.fromfile(open(reionfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        #reshape to subcubes\n",
    "        densE[i] = cubify(densE_i,(img_shape,img_shape,img_shape))\n",
    "        densL[i] = cubify(densL_i,(img_shape,img_shape,img_shape))\n",
    "        reion[i] = cubify(reion_i,(img_shape,img_shape,img_shape))\n",
    "    # add additional axis for number of channels\n",
    "#     densE = densE[..., np.newaxis]\n",
    "#     densL = densL[..., np.newaxis]\n",
    "#     reion = reion[..., np.newaxis]       #these were taken out to fit hyphy data\n",
    "    # scale all fields at once\n",
    "    densE_scaled = scale_field(densE,'eul')\n",
    "    densL_scaled = scale_field(densL,'lag')\n",
    "    reion_scaled = scale_field(reion,'reion')\n",
    "    return densE_scaled,densL_scaled,reion_scaled\n",
    "\n",
    "data = data_feeder(5,32)\n",
    "densE_train = data[0]\n",
    "densL_train = data[1]\n",
    "reion_train = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary to .npy files for use in HyPhy\n",
    "datadir = '/global/cscratch1/sd/tmedan/notebooks/'\n",
    "\n",
    "# Use process like this load to save binary float32 files as .npy into new directory\n",
    "# for HyPhy usage\n",
    "\n",
    "#saves each files from the list of n_files from above as it's own volume in the \n",
    "#correct data format\n",
    "for i in range(5):\n",
    "        # load in simulation\n",
    "        \n",
    "        #arr_eul = np.fromfile(densE_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_density_Eul/dens_'+str(i)+'.npy',densE_train[i])\n",
    "        \n",
    "        \n",
    "        #arr_lag = np.fromfile(densL_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_density_Lag/dens_'+str(i)+'.npy',densL_train[i])\n",
    "        \n",
    "        \n",
    "        #arr_reion = np.fromfile(reion_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_reionization/reion_'+str(i)+'.npy',reion_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import glob\n",
    "listEul = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_density_Eul/dens_*\")) \n",
    "listLag = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_density_Lag/dens_*\"))\n",
    "listReion = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_reionization/reion_*\")) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tr = DataGenerator(listEul[:5],listReion[:5],normy=True,max_num=2,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hy = HyPhy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "DM_field (InputLayer)           [(None, 4, 16, 16, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 4, 16, 16, 16 0           DM_field[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dm_up1 (UpSampling3D)           (None, 4, 32, 32, 32 0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_48 (Conv3D)              (None, 4, 32, 32, 32 436         dm_up1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_49 (Conv3D)              (None, 5, 32, 32, 32 545         conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tau_field (InputLayer)          [(None, 3, 64, 64, 6 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dm_up2 (UpSampling3D)           (None, 5, 64, 64, 64 0           conv3d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 3, 64, 64, 64 0           tau_field[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_50 (Conv3D)              (None, 6, 64, 64, 64 246         dm_up2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_55 (Conv3D)              (None, 3, 64, 64, 64 75          reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_51 (Conv3D)              (None, 6, 64, 64, 64 978         conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_56 (Conv3D)              (None, 4, 64, 64, 64 328         conv3d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 64, 64, 6 0           conv3d_51[0][0]                  \n",
      "                                                                 conv3d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_57 (Conv3D)              (None, 6, 64, 64, 64 3846        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 6, 32, 32, 32 0           conv3d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_58 (Conv3D)              (None, 8, 64, 64, 64 1304        conv3d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_52 (Conv3D)              (None, 8, 32, 32, 32 1304        max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3D)  (None, 8, 32, 32, 32 0           conv3d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 32, 32, 3 0           conv3d_52[0][0]                  \n",
      "                                                                 max_pooling3d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_59 (Conv3D)              (None, 10, 32, 32, 3 20010       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_60 (Conv3D)              (None, 10, 32, 32, 3 2710        conv3d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 8, 16, 16, 16 0           conv3d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 32, 32, 3 128         conv3d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_53 (Conv3D)              (None, 10, 16, 16, 1 2170        max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling3D) (None, 10, 16, 16, 1 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 20, 16, 16, 1 0           conv3d_53[0][0]                  \n",
      "                                                                 max_pooling3d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_61 (Conv3D)              (None, 10, 16, 16, 1 5410        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3D)  (None, 10, 8, 8, 8)  0           conv3d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling3D) (None, 10, 8, 8, 8)  0           conv3d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_54 (Conv3D)              (None, 10, 8, 8, 8)  810         max_pooling3d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling3D) (None, 10, 4, 4, 4)  0           max_pooling3d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3D)  (None, 10, 4, 4, 4)  0           conv3d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 10, 4, 4, 8)  0           max_pooling3d_12[0][0]           \n",
      "                                                                 max_pooling3d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1280)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          163968      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 27)           1755        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 27)           1755        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 27)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 27, 4, 4, 4)  64          lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 37, 4, 4, 4)  0           lambda_1[0][0]                   \n",
      "                                                                 max_pooling3d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_8 (Conv3DTrans (None, 16, 4, 4, 4)  127888      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d (UpSampling3D)    (None, 16, 8, 8, 8)  0           conv3d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_63 (Conv3D)              (None, 16, 8, 8, 8)  6928        up_sampling3d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 16, 16, 16, 1 0           conv3d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 26, 16, 16, 1 0           up_sampling3d_1[0][0]            \n",
      "                                                                 conv3d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_9 (Conv3DTrans (None, 14, 16, 16, 1 9842        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_64 (Conv3D)              (None, 14, 16, 16, 1 1582        conv3d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3D)  (None, 14, 32, 32, 3 0           conv3d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22, 32, 32, 3 0           up_sampling3d_2[0][0]            \n",
      "                                                                 conv3d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_11 (Conv3DTran (None, 12, 32, 32, 3 276         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_65 (Conv3D)              (None, 12, 32, 32, 3 1164        conv3d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3D)  (None, 12, 64, 64, 6 0           conv3d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 18, 64, 64, 6 0           up_sampling3d_3[0][0]            \n",
      "                                                                 conv3d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_66 (Conv3D)              (None, 10, 64, 64, 6 190         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_67 (Conv3D)              (None, 8, 64, 64, 64 10008       conv3d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_68 (Conv3D)              (None, 6, 64, 64, 64 1302        conv3d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_69 (Conv3D)              (None, 10, 64, 64, 6 70          conv3d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_70 (Conv3D)              (None, 6, 64, 64, 64 1626        conv3d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_71 (Conv3D)              (None, 3, 64, 64, 64 489         conv3d_70[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 377,463\n",
      "Trainable params: 377,399\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Hy.cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed in rec_loss and cvae_loss the mean_squared_error from 'objectives'\n",
    "#to tensorflow.compat.v1.metrics.mean_squared_error in an attempy to make \n",
    "#the loss functions work, however, failed to get tf.keras.objectives\n",
    "#to import into my notebook\n",
    "#Therefore tried basic mean squared error here, was able to \n",
    "#run this cell and get training to begin\n",
    "adam = optimizers.Adam(clipnorm=0.9,lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "Hy.cvae.compile(optimizer=adam,loss= 'mse') #Hy.cvae_loss, metrics = [Hy.kl_loss, Hy.rec_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#callbacks...\n",
    "\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau,CSVLogger\n",
    "\n",
    "# logger = CSVLogger(\"./log/csv_log_hyph0318permute_2b\", separator=',', append=True)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"./model_HyPhy0318permute_2b\", monitor='loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected tau_field to have shape (3, 64, 64, 64) but got array with shape (3, 32, 32, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3850b4a30aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m            epochs=50)\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#callbacks=[logger,checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    989\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2471\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    570\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    573\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected tau_field to have shape (3, 64, 64, 64) but got array with shape (3, 32, 32, 32)"
     ]
    }
   ],
   "source": [
    "#training began to run, however, it would not iterate past the first epoch, will be working\n",
    "#on this as possibly an outputted network dimension issue where it cannot compare true\n",
    "#reionization field values to the predicted value\n",
    "hi = Hy.cvae.fit_generator(generator=q_tr,\n",
    "           shuffle=True,\n",
    "           verbose=1,\n",
    "           steps_per_epoch=30,\n",
    "           epochs=50)\n",
    "        #callbacks=[logger,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_generator(\n",
    "    generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None,\n",
    "    validation_data=None, validation_steps=None, validation_freq=1,\n",
    "    class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False,\n",
    "    shuffle=True, initial_epoch=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v1.15.0-gpu",
   "language": "python",
   "name": "tensorflow_gpu_1.15.0_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
