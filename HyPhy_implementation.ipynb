{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#using GPU, comment out if on CPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow \n",
    "# prerequisites\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras import objectives\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import concatenate as concat\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate,Reshape,Flatten\n",
    "from tensorflow.keras.layers import Reshape,UpSampling3D,RepeatVector,Conv3D,MaxPool3D\n",
    "import tensorflow.keras\n",
    "tensorflow.keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "#these are all chosen to make the overall distrubtion roughly unit gaussian... \n",
    "def density_transform(x):\n",
    "    return np.log10(3*x)\n",
    "\n",
    "def velocity_transform(x):\n",
    "    return x/75.0\n",
    "\n",
    "def temp_transform(x):\n",
    "    return (3.82-np.log10(x.clip(max=500000))*3.8) +10\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs_x,list_IDs_y, normy = True,\n",
    "                 to_fit=True, batch_size=32, \n",
    "                 n_channels=1, shuffle=True, inde = [0,1,2],\n",
    "                 max_num = 10, fix_direction=False):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        \n",
    "        :param to_fit: True to return X and y, False to return X only (I think always should be true?)\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param n_channels: number of image channels (should be 1)\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs_x = list_IDs_x #filenames of density fields\n",
    "        self.list_IDs_y = list_IDs_y #filenames of hydro fields\n",
    "        self.max_num = max_num #maximum number of files to use per epoch\n",
    "        self.normy = normy #normalize hydrofield quantities (y/n)?\n",
    "        self.to_fit = to_fit #always True? False not implemented well...\n",
    "        self.batch_size = batch_size \n",
    "        self.n_channels = n_channels #should always be one\n",
    "        self.shuffle = shuffle #shuffle order of boxes\n",
    "        self.st = \"ijk\" #for using various reflection symmetries \n",
    "        self.max = self.__len__()\n",
    "        self.n = 0 #intializing get_item for random rearrangemnt\n",
    "        self.fix_direction = fix_direction #use mirror/reflection symmetries\n",
    "        self.perm = list(itertools.permutations(inde))\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs_x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        #choose random permulation\n",
    "        \n",
    "        #Choose orientation...\n",
    "        y = self.y[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        X  = self.X[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        if self.fix_direction:\n",
    "            ore = (0,1,2)\n",
    "        else:\n",
    "            ore = random.choice(self.perm)\n",
    "            #print(ore)\n",
    "     #   ore = (0, 1, 2)\n",
    "        #print(ore)\n",
    "        # Generate data\n",
    "        #X = self._perm_x(X,ore)\n",
    "        X = X\n",
    "        if self.to_fit:\n",
    "            #print(y.shape)\n",
    "            #y = self._perm_y(y,ore)\n",
    "            y = y\n",
    "            if self.normy:\n",
    "                y = self._norm(y[:,[0,1,4],:,:])\n",
    "            else:\n",
    "                y = y[:,[0,1,4],:,:]\n",
    "            return [y,X],y\n",
    "\n",
    "\n",
    "    def _perm_x(self,hold,i):\n",
    "        #for mirror/reflection symmetries\n",
    "        Q = self.st[i[0]]+self.st[i[1]]+self.st[i[2]]\n",
    "       \n",
    "        t1_x_temp = np.einsum('mnijk->mn'+Q, hold)\n",
    "        t1_x_temp[:,[0,1, 2,3],:,:,:] = t1_x_temp[:,[0,i[0]+1,i[1]+1,i[2]+1],:,:,:]\n",
    "        return t1_x_temp\n",
    "    \n",
    "    def _perm_y(self,hold,i):\n",
    "        #for mirror/reflection symmetries\n",
    "\n",
    "        Q = self.st[i[0]]+self.st[i[1]]+self.st[i[2]]\n",
    "\n",
    "\n",
    "        t1_y_temp = np.einsum('mnijk->mn'+Q, hold)\n",
    "        t1_y_temp[:,[0,1, 2,3,4],:,:,:] = t1_y_temp[:,[0,i[0]+1,i[1]+1,i[2]+1,4],:,:,:]\n",
    "        return t1_y_temp\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs_x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "                # Generate indexes of the batch\n",
    "        #print(indexes)\n",
    "        indexes = self.indexes[0:self.max_num]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_tempx = [self.list_IDs_x[k] for k in indexes]\n",
    "        list_IDs_tempy = [self.list_IDs_y[k] for k in indexes]\n",
    "\n",
    "        self.X = self._generate_X(list_IDs_tempx)\n",
    "        self.y = self._generate_y(list_IDs_tempy)\n",
    "\n",
    "            \n",
    "    def _generate_X(self, list_IDs_temp_x):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        test_x = []\n",
    "        for i in list_IDs_temp_x:\n",
    "            test_x.append([np.load(i)])\n",
    "       \n",
    "        test_x = np.reshape(np.array(test_x),(-1,4096,32,32,32)) #JUST CHANGED FROM 16^3 #second input from 4 to 4096 \n",
    "        #changed from 4 to 0\n",
    "        return test_x\n",
    "\n",
    "    def _generate_y(self, list_IDs_temp_y):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        test_y = []\n",
    "        for i in list_IDs_temp_y:\n",
    "            test_y.append([np.load(i)[:,:,:,:]])\n",
    "        test_y = np.reshape(np.array(test_y),(-1,4096,32,32,32)) ##MATCHED TO FIT MY DATA\n",
    "                            \n",
    "        test_yn = test_y[:,:,:,:] #selecting only one baryon velocity\n",
    "        return test_yn\n",
    "#reducing variance so all variables have similar dynamic range: otherwise loss will be ->inf :'()'\n",
    "\n",
    "    def _norm(self,test_yn):\n",
    "        #if normed = True\n",
    "            \n",
    "            test_yn[:,0] = density_transform(test_yn[:,0].clip(min=0.0001))\n",
    "            test_yn[:,1] = velocity_transform(test_yn[:,1])\n",
    "            test_yn[:,2] = temp_transform(test_yn[:,2].clip(min=0.0001))\n",
    "            \n",
    "            \n",
    "            return test_yn\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "            self.n = 0\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D,Flatten,Conv3DTranspose,concatenate\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate,Reshape,Flatten\n",
    "from tensorflow.keras.layers import Reshape,UpSampling3D,RepeatVector,Conv3D,MaxPool3D, AvgPool3D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "import tensorflow.keras\n",
    "\n",
    "\n",
    "class HyPhy:\n",
    "    \n",
    "    \"\"\"\n",
    "    Class that holds the model for both train (cvae, and associated loss) and generation (gen),\n",
    "    \n",
    "    Currently takes as input just number of hidden dimensions and n_hidden, could \n",
    "    easily make more things free parameters to set...\n",
    "    \n",
    "    Set up slightly strangly to allow (hopefully) seamless switching between training and \n",
    "    generation, as well as changing generation size...\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=128,z_dim=27, edge_clip=10, rec_loss_factor = 1):\n",
    "        self.n_hidden = n_hidden #size of dense layers used to set mu, logvar for latent space\n",
    "        self.z_dim = z_dim #number of latent space dimensions\n",
    "        self.edge_clip = edge_clip #pixels to clip off of reconstructed tau for comparison\n",
    "        self.rec_loss_factor = rec_loss_factor #relative weight of kl loss vs. rec loss\n",
    "        \n",
    "        self.__init_encoder__() #initialize encoder layers\n",
    "        self.__init_decoder__() #initialize decoder layers\n",
    "        \n",
    "        self.__init_cvae__() #creates training model\n",
    "        #self.__init_gen__() #just call HyPhy.gen(), not made by default\n",
    "        \n",
    "    def __init_encoder__(self):\n",
    "        ## dm\n",
    "     \n",
    "        self.inter_up0 = Conv3D(4,3,padding=\"same\",activation=\"selu\")\n",
    "        self.inter_up0p = Conv3D(5,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm1 = Conv3D(6,2,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm1p = Conv3D(6,3,padding=\"same\",activation=\"selu\")\n",
    "\n",
    "        self.encode_dm2 = Conv3D(8,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm3 = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_dm4 = Conv3D(10,2,padding=\"same\",activation=\"selu\") ## linear?\n",
    "\n",
    "        ## tau\n",
    "    \n",
    "        self.encode_tau1 = Conv3D(3,2,padding=\"same\") \n",
    "        self.encode_tau1P = Conv3D(4,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau2 = Conv3D(6,4,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau2P = Conv3D(8,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau3 = Conv3D(10,5,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau3P = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau4 = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "        self.encode_tau4p = Conv3D(10,3,padding=\"same\",activation=\"selu\")\n",
    "    def __init_decoder__(self):\n",
    "        \n",
    "        self.z_decoder2 = Conv3DTranspose(16,6,padding=\"same\",activation=\"selu\")#Dense(n_hidden*3, activation='relu')\n",
    "        self.z_decoder2p = Conv3D(16,3,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3 = Conv3DTranspose(14,3,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3_mix = Conv3DTranspose(14,1,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder3P = Conv3D(14,2,padding=\"same\",activation=\"selu\")\n",
    "        self.z_decoder4 = Conv3DTranspose(12,1,padding=\"same\",activation=\"selu\")#Dense(n_hidden*2, activation='linear')\n",
    "        self.z_decoder4P = Conv3D(12,2,padding=\"same\",activation=\"selu\")#Dense(n_hidden*2, activation='linear')\n",
    "\n",
    "        self.y_decoder = Conv3D(10,1,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "       # self.y_decoder_BN = BatchNormalization()\n",
    "        self.y_decoderP1 = Conv3D(8,5,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP2 = Conv3D(6,3,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP2_mix = Conv3D(10,1,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP3 = Conv3D(6,3,padding=\"same\",activation=\"selu\")#Dense(x_tr.shape[1], activation='linear')\n",
    "        self.y_decoderP4 = Conv3D(3,3,padding=\"same\")#Dense(x_tr.shape[1], activation='linear')\n",
    "    #changed from 4 filters in last decoder #first is 4, second is 3\n",
    "    def encoder_dm(self,dm_box):\n",
    "        \n",
    "        #getting dm field to same size as tau field...\n",
    "        dm0 = UpSampling3D(name=\"dm_up1\")(dm_box)\n",
    "        dm0 = self.inter_up0(dm0)\n",
    "        dm0 = self.inter_up0p(dm0)\n",
    "        dm0 = UpSampling3D((2,2,2),name=\"dm_up2\")(dm0)\n",
    "        \n",
    "        #step one\n",
    "        dm1 = self.encode_dm1(dm0)\n",
    "        dm1 = self.encode_dm1p(dm1)\n",
    "        dm1_p = MaxPool3D()(dm1)\n",
    "\n",
    "        #step two\n",
    "        dm2 = self.encode_dm2(dm1_p)\n",
    "        dm2_p = MaxPool3D()(dm2)\n",
    "      \n",
    "        #step three\n",
    "        dm3 = self.encode_dm3(dm2_p)\n",
    "        dm3_p = MaxPool3D()(dm3)\n",
    "\n",
    "        #step four\n",
    "        dm4 = self.encode_dm4(dm3_p)\n",
    "        dm4 = MaxPool3D()(dm4)\n",
    "        return dm0,dm1,dm2,dm3,dm4\n",
    "    \n",
    "    def encoder_tau(self,x_in,dm0,dm1,dm2,dm3,dm4):\n",
    "        \n",
    "        tau_box = Reshape((3,64,64,64))(x_in) #changed from 64\n",
    "\n",
    "        #step 1\n",
    "        tau1 = self.encode_tau1(tau_box)\n",
    "        tau1 = self.encode_tau1P(tau1)\n",
    "\n",
    "        #merge + step 2\n",
    "        tau1_dm1 = concatenate([dm1,tau1],axis=1)\n",
    "        tau2 = self.encode_tau2(tau1_dm1)\n",
    "        tau2 = self.encode_tau2P(tau2)\n",
    "        tau2_p = MaxPool3D()(tau2)\n",
    "\n",
    "        #merge + step 3\n",
    "\n",
    "        tau2_dm2 = concatenate([dm2,tau2_p],axis=1)\n",
    "        tau3 = self.encode_tau3(tau2_dm2)\n",
    "        tau3 = self.encode_tau3P(tau3)\n",
    "        tau3 = BatchNormalization()(tau3)\n",
    "        tau3_p = MaxPool3D()(tau3)\n",
    "\n",
    "        #merge + step 4\n",
    "        tau3_dm3 = concatenate([dm3,tau3_p],axis=1)\n",
    "        tau4 = self.encode_tau4(tau3_dm3)\n",
    "        tau4 = MaxPool3D()(tau4) #maybe do something else here? more layers?\n",
    "       # tau4 = self.encode_tau4p(tau4)\n",
    "        tau4 = MaxPool3D()(tau4)\n",
    "        return tau4\n",
    "        \n",
    "    def variational_block(self,tau4,dm4):\n",
    "        \n",
    "        x_encoded = concatenate([tau4,dm4])\n",
    "        x_encoded = Flatten()(x_encoded)\n",
    "        x_encoded = Dense(self.n_hidden, activation='selu')(x_encoded)\n",
    "        x_encoded = Dropout(.10)(x_encoded)\n",
    "        x_encoded = Dense(self.n_hidden//2, activation='selu')(x_encoded)\n",
    "        \n",
    "        self.mu = Dense(self.z_dim, activation='linear')(x_encoded)\n",
    "        self.log_var = Dense(self.z_dim, activation='linear')(x_encoded)\n",
    "        \n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            eps = K.random_normal(shape=(K.shape(dm4)[0], self.z_dim), mean=0., stddev=1.0)\n",
    "            return mu + K.exp(log_var/2.) * eps\n",
    "\n",
    "        z = Lambda(sampling, output_shape=(self.z_dim,))([self.mu, self.log_var])\n",
    "        return z\n",
    "    \n",
    "    def decoder(self,z_new, dm0,dm1,dm2,dm3,dm4):\n",
    "            \n",
    "        #dm4_new = Reshape((10,4,4,4))(dm4)\n",
    "\n",
    "        z_cond = concatenate([z_new, dm4],axis=1)\n",
    "    \n",
    "        z_decoded = self.z_decoder2(z_cond)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "        z_decoded = self.z_decoder2p(z_decoded)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm3],axis=1)\n",
    "        z_decoded = self.z_decoder3(z_decoded)\n",
    "        z_decoder = self.z_decoder3_mix(z_decoded)\n",
    "        z_decoded = self.z_decoder3P(z_decoded)\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm2],axis=1)\n",
    "\n",
    "        z_decoded = self.z_decoder4(z_decoded)\n",
    "        z_decoded = self.z_decoder4P(z_decoded)\n",
    "\n",
    "        z_decoded = UpSampling3D()(z_decoded)\n",
    "\n",
    "        z_decoded = concatenate([z_decoded,dm1],axis=1)\n",
    "\n",
    "        y0 = self.y_decoder(z_decoded)\n",
    "       # y0 = self.y_decoder_BN(y0)\n",
    "        z_decoded = concatenate([y0,dm0],axis=1)\n",
    "        y0 = self.y_decoderP1(y0)\n",
    "        y0 = self.y_decoderP2(y0)\n",
    "        y0 = self.y_decoderP2_mix(y0)\n",
    "        y0 = self.y_decoderP3(y0)\n",
    "        y0 = MaxPool3D()(y0)  #ADDED MAX POOL TO HAVE OUTPUT = TARGET\n",
    "        y = self.y_decoderP4(y0)\n",
    "        return y\n",
    "    \n",
    "    def __init_cvae__(self):\n",
    "        \n",
    "        condition = Input(shape=(4096,32,32,32),name=\"DM_field\") #16^3 #changed from 4 to 4096\n",
    "        x_in = Input(shape=(3,32,32,32),name=\"tau_field\") #64^3 #changed from 3 to zero\n",
    "        dm_box = Reshape((4,16,16,16))(condition) #16^3\n",
    "\n",
    "        dm0,dm1,dm2,dm3,dm4 = self.encoder_dm(dm_box)\n",
    "        tau4 = self.encoder_tau(x_in,dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        z = self.variational_block(tau4,dm4)\n",
    "        \n",
    "        z_new = Lambda(lambda x: tensorflow.tensordot(x, K.ones((4,4,4)), axes=0))(z)\n",
    "        \n",
    "        y = self.decoder(z_new, dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        self.cvae = Model([x_in, condition], y)\n",
    "    \n",
    "    def cvae_loss(self,x,y):\n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "\n",
    "        reconstruction_loss = tensorflow.compat.v1.metrics.mean_squared_error(x_f, y_f)*16**3*4**3*self.rec_loss_factor\n",
    "        kl_loss = 0.5 * K.sum(K.square(self.mu) + K.exp(self.log_var) - self.log_var - 1, axis = -1)\n",
    "        loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return loss\n",
    "    \n",
    "    def kl_loss(self,x,y):\n",
    "        # just for debugging\n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        kl_loss = 0.5 * K.sum(K.square(self.mu) + K.exp(self.log_var) - self.log_var - 1, axis = -1)\n",
    "        return kl_loss\n",
    "    \n",
    "    def rec_loss(self,x,y):    \n",
    "    \n",
    "        ec = int(self.edge_clip)\n",
    "        x_f = K.flatten(x[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "        y_f = K.flatten(y[:,:,ec:-1*ec,ec:-1*ec,ec:-1*ec])\n",
    "\n",
    "        reconstruction_loss = tensorflow.compat.v1.metrics.mean_squared_error(x_f, y_f)*16**3*4**3*self.rec_loss_factor\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def generator(self,size1 = 32,reduced_len=8):\n",
    "        \n",
    "        _condition = Input(shape=(4096,size1,size1,size1),name=\"DM_field\") #changed from 4\n",
    "        _z = Input(shape=(self.z_dim,))\n",
    "        #x_in = Input(shape=(3,64,64,64),name=\"tau_field\")\n",
    "        \n",
    "        dm0,dm1,dm2,dm3,dm4 = self.encoder_dm(_condition)\n",
    "        \n",
    "        _z_new = Lambda(lambda x: tensorflow.tensordot(x, K.ones((reduced_len,reduced_len,reduced_len)), axes=0))(_z)\n",
    "\n",
    "        _y = self.decoder(_z_new, dm0,dm1,dm2,dm3,dm4)\n",
    "        \n",
    "        self.gen = Model([_condition,_z], _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing of HyPhy_ density files\n",
    "\n",
    "#Below are two functions used in the pre-processing of my density fields BEFORE\n",
    "#they are saved to .npy files\n",
    "\n",
    "def scale_field(field_in, field_type, inv=False):\n",
    "    '''\n",
    "    function to scale Lagrangian, Eulerian, and reionization fields\n",
    "    from their physical units to the range (0,1), and invert this scaling if needed.\n",
    "    Can choose other scalings from e.g. https://en.wikipedia.org/wiki/Feature_scaling - right now does min-max scaling.\n",
    "    Keyword arguments:\n",
    "    field_in -- the field to scale\n",
    "    field_type -- type of field: 'lag', 'eul', or 'reion'\n",
    "    inv -- False=forward scaling from physical units to normalized, True=inverse\n",
    "    \"\"\"\n",
    "    '''\n",
    "    field_types =  ['eul', 'lag', 'reion']\n",
    "    field_mins  = np.array([ -1.        , -23.45174217,   6.69999981])\n",
    "    field_maxs  = np.array([11.14400196, 22.52767944, 16.        ])\n",
    "    ind = field_types.index(field_type)\n",
    "    fmin = field_mins[ind]\n",
    "    fmax = field_maxs[ind]\n",
    "    #min max scaling\n",
    "    if not inv:\n",
    "        return (field_in - fmin)/(fmax - fmin)\n",
    "    if inv:\n",
    "        return field_in * (fmax - fmin) + fmin\n",
    "    \n",
    "def cubify(arr, newshape):\n",
    "    '''stolen from https://stackoverflow.com/questions/42297115/numpy-split-cube-into-cubes'''\n",
    "    oldshape = np.array(arr.shape)\n",
    "    repeats = (oldshape / newshape).astype(int)\n",
    "    tmpshape = np.column_stack([repeats, newshape]).ravel()\n",
    "    order = np.arange(len(tmpshape))\n",
    "    order = np.concatenate([order[::2], order[1::2]])\n",
    "    \n",
    "    # newshape must divide oldshape evenly or else ValueError will be raised\n",
    "    return arr.reshape(tmpshape).transpose(order).reshape(-1, *newshape)\n",
    "\n",
    "def uncubify(arr, oldshape):\n",
    "    '''stolen from https://stackoverflow.com/questions/42297115/numpy-split-cube-into-cubes'''\n",
    "    N, newshape = arr.shape[0], arr.shape[1:]\n",
    "    oldshape = np.array(oldshape)    \n",
    "    repeats = (oldshape / newshape).astype(int)\n",
    "    tmpshape = np.concatenate([repeats, newshape])\n",
    "    order = np.arange(len(tmpshape)).reshape(2, -1).ravel(order='F')\n",
    "    return arr.reshape(tmpshape).transpose(order).reshape(oldshape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "#Here we have the data processing I used on my density files to turn it \n",
    "#into files of size (n_files,4096,32,32,32)\n",
    "\n",
    "#NO NEED TO RUN THIS AGAIN\n",
    "datadir = '/global/cscratch1/sd/tmedan/notebooks/'\n",
    "def data_feeder(n_files, img_shape):\n",
    "    sim_size  =  512\n",
    "    n_subcube = int((sim_size//img_shape)**3)\n",
    "    # make empty array first.\n",
    "    densE = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    densL = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    reion = np.zeros((n_files, n_subcube, img_shape, img_shape, img_shape))\n",
    "    for i in range(n_files):\n",
    "        # load in simulation\n",
    "        densEfile_i = datadir+'density_Eul/dens_{:02d}'.format(i)\n",
    "        densE_i = np.fromfile(open(densEfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        \n",
    "        densLfile_i = datadir+'density_Lag/dens_{:02d}'.format(i)\n",
    "        densL_i = np.fromfile(open(densLfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        \n",
    "        reionfile_i = datadir+'reionization/reion_{:02d}'.format(i)\n",
    "        reion_i = np.fromfile(open(reionfile_i),count=sim_size**3,dtype=np.float32).reshape(sim_size, sim_size, sim_size)\n",
    "        #reshape to subcubes\n",
    "        densE[i] = cubify(densE_i,(img_shape,img_shape,img_shape))\n",
    "        densL[i] = cubify(densL_i,(img_shape,img_shape,img_shape))\n",
    "        reion[i] = cubify(reion_i,(img_shape,img_shape,img_shape))\n",
    "    # add additional axis for number of channels\n",
    "#     densE = densE[..., np.newaxis]\n",
    "#     densL = densL[..., np.newaxis]\n",
    "#     reion = reion[..., np.newaxis]       #these were taken out to fit hyphy data\n",
    "    # scale all fields at once\n",
    "    densE_scaled = scale_field(densE,'eul')\n",
    "    densL_scaled = scale_field(densL,'lag')\n",
    "    reion_scaled = scale_field(reion,'reion')\n",
    "    return densE_scaled,densL_scaled,reion_scaled\n",
    "\n",
    "data = data_feeder(5,32)\n",
    "densE_train = data[0]\n",
    "densL_train = data[1]\n",
    "reion_train = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary to .npy files for use in HyPhy\n",
    "datadir = '/global/cscratch1/sd/tmedan/notebooks/'\n",
    "\n",
    "# Use process like this load to save binary float32 files as .npy into new directory\n",
    "# for HyPhy usage\n",
    "\n",
    "#saves each files from the list of n_files from above as it's own volume in the \n",
    "#correct data format\n",
    "for i in range(5):\n",
    "        # load in simulation\n",
    "        \n",
    "        #arr_eul = np.fromfile(densE_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_density_Eul/dens_'+str(i)+'.npy',densE_train[i])\n",
    "        \n",
    "        \n",
    "        #arr_lag = np.fromfile(densL_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_density_Lag/dens_'+str(i)+'.npy',densL_train[i])\n",
    "        \n",
    "        \n",
    "        #arr_reion = np.fromfile(reion_train[i],dtype=np.float32)\n",
    "        np.save('HyPhy_reionization/reion_'+str(i)+'.npy',reion_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import glob\n",
    "listEul = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_density_Eul/dens_*\")) \n",
    "listLag = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_density_Lag/dens_*\"))\n",
    "listReion = sorted(glob.glob(\"/global/cscratch1/sd/tmedan/notebooks/HyPhy_reionization/reion_*\")) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tr = DataGenerator(listEul[:5],listReion[:5],normy=True,max_num=2,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/common/software/tensorflow/gpu-tensorflow/1.15.0-py37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "Hy = HyPhy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "DM_field (InputLayer)           [(None, 4096, 32, 32 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 4, 16, 16, 16 0           DM_field[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dm_up1 (UpSampling3D)           (None, 4, 32, 32, 32 0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d (Conv3D)                 (None, 4, 32, 32, 32 436         dm_up1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 5, 32, 32, 32 545         conv3d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tau_field (InputLayer)          [(None, 3, 32, 32, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dm_up2 (UpSampling3D)           (None, 5, 64, 64, 64 0           conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 64, 64, 64 0           tau_field[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 6, 64, 64, 64 246         dm_up2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 3, 64, 64, 64 75          reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 6, 64, 64, 64 978         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 4, 64, 64, 64 328         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 64, 64, 6 0           conv3d_3[0][0]                   \n",
      "                                                                 conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 6, 64, 64, 64 3846        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D)    (None, 6, 32, 32, 32 0           conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 8, 64, 64, 64 1304        conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 8, 32, 32, 32 1304        max_pooling3d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 8, 32, 32, 32 0           conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 32, 32, 3 0           conv3d_4[0][0]                   \n",
      "                                                                 max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 10, 32, 32, 3 20010       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 10, 32, 32, 3 2710        conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 8, 16, 16, 16 0           conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 32, 32, 3 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 10, 16, 16, 1 2170        max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 10, 16, 16, 1 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20, 16, 16, 1 0           conv3d_5[0][0]                   \n",
      "                                                                 max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 10, 16, 16, 1 5410        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 10, 8, 8, 8)  0           conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 10, 8, 8, 8)  0           conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 10, 8, 8, 8)  810         max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3D)  (None, 10, 4, 4, 4)  0           max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, 10, 4, 4, 4)  0           conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 10, 4, 4, 8)  0           max_pooling3d_7[0][0]            \n",
      "                                                                 max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1280)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          163968      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 27)           1755        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 27)           1755        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 27)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 27, 4, 4, 4)  64          lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 37, 4, 4, 4)  0           lambda_1[0][0]                   \n",
      "                                                                 max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose (Conv3DTranspo (None, 16, 4, 4, 4)  127888      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d (UpSampling3D)    (None, 16, 8, 8, 8)  0           conv3d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 16, 8, 8, 8)  6928        up_sampling3d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 16, 16, 16, 1 0           conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 26, 16, 16, 1 0           up_sampling3d_1[0][0]            \n",
      "                                                                 conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTrans (None, 14, 16, 16, 1 9842        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 14, 16, 16, 1 1582        conv3d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3D)  (None, 14, 32, 32, 3 0           conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 22, 32, 32, 3 0           up_sampling3d_2[0][0]            \n",
      "                                                                 conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTrans (None, 12, 32, 32, 3 276         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 12, 32, 32, 3 1164        conv3d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3D)  (None, 12, 64, 64, 6 0           conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 18, 64, 64, 6 0           up_sampling3d_3[0][0]            \n",
      "                                                                 conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 10, 64, 64, 6 190         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 8, 64, 64, 64 10008       conv3d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 6, 64, 64, 64 1302        conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 10, 64, 64, 6 70          conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 6, 64, 64, 64 1626        conv3d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3D)  (None, 6, 32, 32, 32 0           conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 3, 32, 32, 32 489         max_pooling3d_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 377,463\n",
      "Trainable params: 377,399\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Hy.cvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed in rec_loss and cvae_loss the mean_squared_error from 'objectives'\n",
    "#to tensorflow.compat.v1.metrics.mean_squared_error in an attempy to make \n",
    "#the loss functions work, however, failed to get tf.keras.objectives\n",
    "#to import into my notebook\n",
    "#Therefore tried basic mean squared error here, was able to \n",
    "#run this cell and get training to begin\n",
    "adam = optimizers.Adam(clipnorm=0.9,lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "Hy.cvae.compile(optimizer=adam,loss= 'mse') #Hy.cvae_loss, metrics = [Hy.kl_loss, Hy.rec_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#callbacks...\n",
    "\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau,CSVLogger\n",
    "\n",
    "# logger = CSVLogger(\"./log/csv_log_hyph0318permute_2b\", separator=',', append=True)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"./model_HyPhy0318permute_2b\", monitor='loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training began to run, however, it would not iterate past the first epoch, will be working\n",
    "#on this as possibly an outputted network dimension issue where it cannot compare true\n",
    "#reionization field values to the predicted value\n",
    "hi = Hy.cvae.fit_generator(generator=q_tr,\n",
    "           shuffle=True,\n",
    "           verbose=1,\n",
    "           steps_per_epoch=30,\n",
    "           epochs=50)\n",
    "        #callbacks=[logger,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v1.15.0-gpu",
   "language": "python",
   "name": "tensorflow_gpu_1.15.0_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
